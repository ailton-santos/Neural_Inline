# -*- coding: utf-8 -*-
"""Neural_InLine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qAyjXLKBsJDINfgxCPkSJ2iGNcfSvvF2
"""

# Importar bibliotecas
!pip install keras-tuner
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# 1. Carregar e processar os dados
# Simulando um dataset industrial (substitua pelo seu dataset real)
data = {
    'temperature': np.random.uniform(30, 100, 1000),
    'vibration': np.random.uniform(0, 1, 1000),
    'pressure': np.random.uniform(50, 200, 1000),
    'humidity': np.random.uniform(10, 90, 1000),
    'machine_failure': np.random.choice([0, 1], size=1000, p=[0.95, 0.05])
}
df = pd.DataFrame(data)

# Verificar dados ausentes
df.fillna(df.mean(), inplace=True)

# Gerar novos recursos
# Exemplo: Relação entre pressão e temperatura
df['pressure_temp_ratio'] = df['pressure'] / (df['temperature'] + 1)

# Separar variáveis independentes (X) e dependentes (y)
X = df.drop(columns=['machine_failure'])
y = df['machine_failure']

# Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalizar os dados
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 2. Criar a função de construção do modelo para o KerasTuner
def build_model(hp):
    model = Sequential([
        Dense(hp.Int('units_1', min_value=32, max_value=128, step=32), activation='relu', input_dim=X_train.shape[1]),
        Dropout(hp.Choice('dropout_rate', [0.2, 0.3, 0.4])),
        Dense(hp.Int('units_2', min_value=16, max_value=64, step=16), activation='relu'),
        Dropout(hp.Choice('dropout_rate_2', [0.2, 0.3, 0.4])),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=hp.Choice('optimizer', ['adam', 'rmsprop']),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

# 3. Ajuste de hiperparâmetros com KerasTuner
tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    directory='my_dir',
    project_name='tune_nn'
)

# Executar busca pelos melhores hiperparâmetros
tuner.search(X_train, y_train, epochs=50, validation_split=0.2, verbose=1)

# Obter os melhores hiperparâmetros
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print("Melhores hiperparâmetros:")
print(best_hps.values)

# Construir o modelo com os melhores hiperparâmetros
model = tuner.hypermodel.build(best_hps)

# 4. Treinar o modelo com os melhores hiperparâmetros
history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=1)

# 5. Avaliar no conjunto de teste
y_pred = (model.predict(X_test) > 0.5).astype(int)
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred))

# Matriz de confusão
cm = confusion_matrix(y_test, y_pred)
print("\nMatriz de Confusão:")
print(cm)

# 6. Visualização de desempenho
plt.plot(history.history['accuracy'], label='Acurácia')
plt.plot(history.history['val_accuracy'], label='Validação - Acurácia')
plt.plot(history.history['loss'], label='Perda')
plt.plot(history.history['val_loss'], label='Validação - Perda')
plt.xlabel('Epoch')
plt.ylabel('Valor')
plt.legend()
plt.show()

# 7. Fazer previsão com dados novos
new_data = np.array([[70, 0.3, 120, 45, 1.5]])  # Temperatura, Vibração, Pressão, Umidade, Relação Pressão/Temp
new_data_scaled = scaler.transform(new_data)
prediction = model.predict(new_data_scaled)
print(f"Chance de falha: {'Sim' if prediction[0] > 0.5 else 'Não'}")

